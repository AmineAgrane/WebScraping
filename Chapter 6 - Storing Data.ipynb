{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media Files\n",
    "\n",
    "Media files are generaly stored in twi distinct ways : \n",
    "   * By downloading directly the file.\n",
    "   * By reference (storing the url where the file is located)\n",
    "   \n",
    "Each approch has its advantages and disadvantages, the final choice on the context of the application.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download media files from a Website\n",
    "Let's write a little fuction that we can use to download a specific image from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "url = 'http://www.wonaruto.com/'\n",
    "path = 'h1 > a > img'\n",
    "\n",
    "def download_image(url, path):\n",
    "    html = urlopen(url)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    imageLocation = bs.select_one(path)['src']\n",
    "    urlretrieve (imageLocation, 'logo.jpg')\n",
    "\n",
    "download_image(url, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well if you need to download only a single files. But most scrapers don’t download a single file and call\n",
    "it a day. The following downloads all internal files, linked to by any tag’s src attribute, from the home page of http://pythonscraping.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://pythonscraping.com/https://pythonscraping.com/wp-includes/js/jquery/jquery.min.js?ver=7c03884fd15f65fbf9063b08180d457d\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-includes/js/jquery/jquery-migrate.min.js?ver=7c03884fd15f65fbf9063b08180d457d\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/plugins/pagelayer/js/combined.js?ver=7c03884fd15f65fbf9063b08180d457d\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/home1.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/home2.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work1.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work9.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work9.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/work8.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/testimonial3.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/testimonial3.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/testimonial3.jpg\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/uploads/2021/08/logo01.png\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-content/themes/popularfx/js/navigation.js?ver=7c03884fd15f65fbf9063b08180d457d\n",
      "http://pythonscraping.com/https://pythonscraping.com/wp-includes/js/wp-embed.min.js?ver=7c03884fd15f65fbf9063b08180d457d\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 123] La syntaxe du nom de fichier, de répertoire ou de volume est incorrecte: 'downloaded/https:'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5aed8f85a321>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetDownloadPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownloadDirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-5aed8f85a321>\u001b[0m in \u001b[0;36mgetDownloadPath\u001b[1;34m(baseUrl, absoluteUrl, downloadDirectory)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] La syntaxe du nom de fichier, de répertoire ou de volume est incorrecte: 'downloaded/https:'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "downloadDirectory = 'downloaded'\n",
    "baseUrl = 'http://pythonscraping.com'\n",
    "\n",
    "def getAbsoluteURL(baseUrl, source):\n",
    "    if source.startswith('http://www.'):\n",
    "        url = 'http://{}'.format(source[11:])\n",
    "    elif source.startswith('http://'):\n",
    "        url = source\n",
    "    elif source.startswith('www.'):\n",
    "        url = source[4:]\n",
    "        url = 'http://{}'.format(source)\n",
    "    else:\n",
    "        url = '{}/{}'.format(baseUrl, source)\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
    "    path = absoluteUrl.replace('www.', '')\n",
    "    path = path.replace(baseUrl, '')\n",
    "    path = downloadDirectory+path\n",
    "    directory = os.path.dirname(path)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    return path\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "downloadList = bs.findAll(src=True)\n",
    "\n",
    "for download in downloadList:\n",
    "    fileUrl = getAbsoluteURL(baseUrl, download['src'])\n",
    "    if fileUrl is not None:\n",
    "        print(fileUrl)\n",
    "\n",
    "urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python & MySQL\n",
    "\n",
    "We'll use the `PyMySQL` package to acces our previously defined database from a python script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Test page title', 'This is some test page content. It can be up to 10,000 characters\\nlong.', datetime.datetime(2021, 12, 3, 11, 45, 22))\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# set creditentials data\n",
    "password='Fwtk&SSPSb2CjBt^wYd7'\n",
    "host = '127.0.0.1'\n",
    "\n",
    "# make a connection to the SQL databse\n",
    "connection = pymysql.connect(host=host, user='root', passwd=password, db='mysql')\n",
    "cur = connection.cursor()\n",
    "\n",
    "\n",
    "# select the database to use\n",
    "cur.execute('USE scraping')\n",
    "\n",
    "# execute a SQL request\n",
    "cur.execute('SELECT * FROM pages WHERE id=1')\n",
    "print(cur.fetchone())\n",
    "\n",
    "# close connection\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a simple program that scrapes data from wikipedia and insert it inside our database : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/MTV_Movie_%26_TV_Awards\n",
      "/wiki/Bill_Daily\n",
      "/wiki/United_Press_International\n",
      "/wiki/Agenzia_Nazionale_Stampa_Associata\n",
      "/wiki/Bresciaoggi\n",
      "/wiki/Gazzetta_di_Parma\n",
      "/wiki/MF_Milano_Finanza\n",
      "/wiki/La_Tribuna_di_Treviso\n",
      "/wiki/Gazzetta_del_Sud\n",
      "/wiki/Il_Giornale\n",
      "/wiki/Milan\n",
      "/wiki/Media_(communication)\n",
      "/wiki/Information\n",
      "/wiki/Ed_Hawkins_(climatologist)\n",
      "/wiki/National_Centre_for_Atmospheric_Science\n",
      "/wiki/Government_of_the_United_Kingdom\n",
      "/wiki/Bank_of_England\n",
      "/wiki/Big_City_Plan\n",
      "/wiki/Economy_of_Birmingham\n",
      "/wiki/Architecture_of_Birmingham\n",
      "/wiki/Ian_Simpson_(architect)\n",
      "/wiki/Foster_and_Partners\n",
      "/wiki/Thames_Hub_integrated_infrastructure_vision\n",
      "/wiki/Kent_International_Airport\n",
      "/wiki/Sky_Wings\n",
      "/wiki/Athens_International_Airport\n",
      "/wiki/Geographic_coordinate_system\n",
      "/wiki/Crustal_deformation\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/ISO/IEC_9995\n",
      "/wiki/ISO/IEC_42010\n",
      "/wiki/ISO_518\n",
      "/wiki/Hyperfocal_distance\n",
      "/wiki/F-number\n",
      "/wiki/Digital_image_processing\n",
      "/wiki/Luminance\n",
      "/wiki/Lambert_(unit)\n",
      "/wiki/Skot_(unit)\n",
      "/wiki/Foot-lambert\n",
      "/wiki/Square_inch\n",
      "/wiki/Thousandth_of_an_inch\n",
      "/wiki/Hoppus\n",
      "/wiki/Guinean_units_of_measurement\n",
      "/wiki/Romanian_units_of_measurement\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0bd60cf479bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m          \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewArticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m          \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewArticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-0bd60cf479bb>\u001b[0m in \u001b[0;36mgetLinks\u001b[1;34m(articleUrl)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://en.wikipedia.org'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0marticleUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'read'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m        \u001b[1;31m# It's a file-type object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         elif len(markup) <= 256 and (\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readall_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                 \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0bd60cf479bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import pymysql\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# set creditentials data\n",
    "password='Fwtk&SSPSb2CjBt^wYd7'\n",
    "host = '127.0.0.1'\n",
    "\n",
    "# make a connection to the SQL databse\n",
    "connection = pymysql.connect(host=host, user='root', passwd=password, db='mysql', charset='utf8')\n",
    "cur = connection.cursor()\n",
    "cur.execute('USE scraping')\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "\n",
    "def store(title, content):\n",
    "    \"\"\" takes in 2 string variables, title and content, and adds them to an INSERT statment to the PAGES table\"\"\"\n",
    "    cur.execute('INSERT INTO pages (title, content) VALUES (\"%s\", \"%s\")', (title, content))\n",
    "    cur.connection.commit()\n",
    "\n",
    "    \n",
    "def getLinks(articleUrl):\n",
    "    \"\"\" takes a wikipedia link, extract it title and content, call store function, return a lsit of \n",
    "        all other wiki's articles links found on this page\"\"\"\n",
    "    html = urlopen('http://en.wikipedia.org'+articleUrl)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    content = bs.find('div', {'id':'mw-content-text'}).find('p').get_text()\n",
    "    store(title, content)\n",
    "    return bs.find('div', {'id':'bodyContent'}).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "try:\n",
    "    while len(links) > 0:\n",
    "         newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "         print(newArticle)\n",
    "         links = getLinks(newArticle)\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test page title',)\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT title FROM pages')\n",
    "print(cur.fetchone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebScraping",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
