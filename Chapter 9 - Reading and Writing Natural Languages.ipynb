{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Data\n",
    "\n",
    "We write a code that will the text data and returns a Counter object with all 2-grams in the text :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of the', 213), ('in the', 65), ('to the', 61), ('by the', 41), ('the constitution', 34), ('of our', 29), ('to be', 26), ('the people', 24), ('from the', 24), ('that the', 23)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def cleanInput(content):\n",
    "    content = content.lower()\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode('ascii', 'ignore')\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n",
    "    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)\n",
    "\n",
    "\n",
    "content = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(), 'utf-8')\n",
    "ngrams = getNgrams(content, 2)\n",
    "print(ngrams.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these 2-grams, “the constitution” seems like a reasonably popular subject in the speech, but “of the,” “in the,” and “to the” don’t seem especially noteworthy. How can you automatically get rid of unwanted words in an accurate way? Here we intoroduce the concept of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('united states', 10), ('executive department', 4), ('general government', 4), ('called upon', 3), ('chief magistrate', 3), ('legislative body', 3), ('same causes', 3), ('government should', 3), ('whole country', 3), ('was observable', 2)]\n"
     ]
    }
   ],
   "source": [
    "def isCommon(ngram):\n",
    "    StopWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', \n",
    "                 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', \n",
    "                 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', \n",
    "                 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', \n",
    "                 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', \n",
    "                 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', \n",
    "                 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n",
    "    StopWords = [word.lower() for word in StopWords]\n",
    "    for word in ngram:\n",
    "        if word in StopWords:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        if not isCommon(content[i:i+n]):\n",
    "            output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "ngrams = getNgrams(content, 2)\n",
    "print(ngrams.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models\n",
    "Again using the inauguration speech of William Henry Harrison analyzed in the previous example, you can write the following code that generates arbitrarily long Markov chains (with the chain length set to 100) based on the structure of its text The output of this code changes every time it is run, but here’s an example of the uncannily nonsensical text it will generate:\n",
    "\n",
    "\n",
    "The function `buildWordDict` takes in the string of text then does some cleaning and formatting, removing quotes and putting\n",
    "spaces around punctuation so it's treated as a separate word. After this, it builds a two-dimensional dictionary—a dictionary of dictionaries—that has the following form :\n",
    "\n",
    "    '''\n",
    "    {word_a : {word_b : 2, word_c : 1, word_d : 1}, \n",
    "    word_e : {word_b : 5, word_d : 2},...}\n",
    "    '''\n",
    "    \n",
    "In this example dictionary, “word_a” was found four times, two instances of which were followed by “word_b,” one instance followed by “word_c,” and one instance followed by “word_d.” “Word_e” was followed seven times, five times by “word_b” and\n",
    "twice by “word_d.”\n",
    "\n",
    "If we were to draw a node model of this result, the node representing word_a would have a 50% arrow pointing toward “word_b” (which followed it two out of four times), a 25% arrow pointing toward “word_c,” and a 25% arrow pointing toward “word_d.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I refer to the execution of power in attributing the States accept a profound reverence for the dictates of want of the errors there is suffered to constitute a spirit which our free people of a thorough examination of the servant , not yet , the Convention that of no member of each were alarmed at least to communicate information and safety from his growth and sense of disunion , is the departments , virtually subject the greatest of encroachments of interest , discord , that which belong to recommend measures so long exist . In other person . There is\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "\n",
    "def wordListSum(wordList):\n",
    "    sum = 0\n",
    "    for word, value in wordList.items():\n",
    "        sum += value\n",
    "    return sum\n",
    "\n",
    "def retrieveRandomWord(wordList):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "def buildWordDict(text):\n",
    "    # Remove newlines and quotes\n",
    "    text = text.replace('\\n', ' ');\n",
    "    text = text.replace('\"', '');\n",
    "\n",
    "    # Add space between punctuation marks so that they will be included in the Markov chain\n",
    "    punctuation = [',','.',';',':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, ' {} '.format(symbol));\n",
    "\n",
    "    words = text.split(' ')\n",
    "    # Filter out empty words\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    wordDict = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] not in wordDict:\n",
    "                # Create a new dictionary for this word\n",
    "            wordDict[words[i-1]] = {}\n",
    "        if words[i] not in wordDict[words[i-1]]:\n",
    "            wordDict[words[i-1]][words[i]] = 0\n",
    "        wordDict[words[i-1]][words[i]] += 1\n",
    "    return wordDict\n",
    "\n",
    "text = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt')\n",
    "          .read(), 'utf-8')\n",
    "wordDict = buildWordDict(text)\n",
    "\n",
    "#Generate a Markov chain of length 100\n",
    "length = 100\n",
    "chain = ['I']\n",
    "for i in range(0, length):\n",
    "    newWord = retrieveRandomWord(wordDict[chain[-1]])\n",
    "    chain.append(newWord)\n",
    "\n",
    "print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis with NLTK\n",
    "NLTK is great for generating statistical information about word counts, word frequency, and word diversity in sections of text. Analysis with NLTK always starts with the `Text` object. `Text` objects can be created from simple Python strings in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Here is some not very interesting text...>\n",
      "['Here', 'is', 'some', 'not', 'very', 'interesting', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize('Here is some not very interesting text')\n",
    "text = Text(tokens)\n",
    "print(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input for the word_tokenize function can be any Python text string. If you don’t hav long strings handy but still want to play around with the features, NLTK has quite a few books already built into the library, which can be accessed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text6\n",
    "print(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count unique words\n",
    "Text objects can be manipulated much like normal Python arrays. Using this property, you can count the number of unique words in a text and compare it against the total number of words. It shows that each word in the script was used about eight times on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.833333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text6)/len(set(text6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words frequency\n",
    "We can also put the text into a frequency distribution object to determine some of the most common words and the frequencies for various words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':', 1197),\n",
       " ('.', 816),\n",
       " ('!', 801),\n",
       " (',', 731),\n",
       " (\"'\", 421),\n",
       " ('[', 319),\n",
       " (']', 312),\n",
       " ('the', 299),\n",
       " ('I', 255),\n",
       " ('ARTHUR', 225)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(text6)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram and 3-gram\n",
    "We can create, search, and list N-grams extremely easily in NLTK. Here, the ngrams function is called to break a text object into n-grams of any size, governed by the second parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ARTHUR', ':'), 217),\n",
       " ((\"'\", 's'), 140),\n",
       " ((']', '['), 94),\n",
       " (('!', '['), 82),\n",
       " ((':', 'Oh'), 82),\n",
       " (('Oh', ','), 79),\n",
       " ((\"'\", 't'), 77)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "bigrams = bigrams(text6)\n",
    "\n",
    "bigramsDist = FreqDist(bigrams)\n",
    "bigramsDist.most_common(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((':', '[', 'singing', ']'), 25),\n",
       " (('GUARD', '#', '1', ':'), 23),\n",
       " (('VILLAGER', '#', '1', ':'), 22),\n",
       " (('Hello', '.', 'Hello', '.'), 22),\n",
       " (('.', 'Hello', '.', 'Hello'), 21),\n",
       " (('SOLDIER', '#', '1', ':'), 18),\n",
       " (('witch', '!', 'A', 'witch'), 17)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "fourgrams = ngrams(text6, 4)\n",
    "fourgramsDist = FreqDist(fourgrams)\n",
    "fourgramsDist.most_common(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicographical Analysis with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebScraping",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
