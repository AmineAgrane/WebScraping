{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Different Website Layouts\n",
    "In most cases of web crawling, you’re not looking to collect data from sites you’ve never seen before, but from a few, or a few dozen, websites that are preselected by a human. This means that you don’t need to use complicated algorithms or machine learning to detect which text on the page “looks most like a title” or which is probably the “main content.” You can determine what these elements are manually.\n",
    "\n",
    "The most obvious approach is to write a separate page parser for each website. Each might take in a URL, string, or BeautifulSoup object, and return a Python object for the thing that was scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Why AI is just automation \n",
      " URL: https://www.brookings.edu/research/why-ai-is-just-automation/\n",
      "\n",
      "Title: The Men Who Want to Live Forever \n",
      " URL: https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Content:\n",
    "    \"\"\" Class to define our data model that contains 3 elements\"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    body = bs.find(\"div\",{\"class\",\"post-body\"}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    lines = bs.find_all(\"p\", {\"class\":\"story-content\"})\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "\n",
    "content = scrapeBrookings('https://www.brookings.edu/research/why-ai-is-just-automation/')\n",
    "print('Title: {}'.format(content.title), '\\n', 'URL: {}\\n'.format(content.url))\n",
    "#print(content.body)\n",
    "\n",
    "content = scrapeNYTimes('https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html')\n",
    "print('Title: {}'.format(content.title), '\\n', 'URL: {}\\n'.format(content.url))\n",
    "#print(content.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the code more Object Oriented\n",
    "\n",
    "What we see from the previous code is that the only real site-dependent variables here are the CSS selectors **used to obtain each piece of information.** BeautifulSoup’s `find` and `find_all` functions take in 2 arguments, a tag string and a dictionary of key/value attributes; so we can pass these arguments in as parameters that define the location of the target data.\n",
    "\n",
    "\n",
    "To make things even more convenient, rather than dealing with all of these tag arguments and key/value pairs, you can use the BeautifulSoup `select` function with a single string CSS selector for each piece of information you want to collect and put all of these selectors in a dictionary object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"  Data model for this project \"\"\"\n",
    "    \n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\" Flexible printing function controls output \"\"\"\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "\n",
    "class Website:\n",
    "    \"\"\" \n",
    "    Contains information about website structure. The Website class doesn't store the collected \n",
    "    data from the pages, but stores instructions about how to collect that data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
