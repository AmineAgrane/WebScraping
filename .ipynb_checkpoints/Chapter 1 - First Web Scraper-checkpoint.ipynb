{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `BeautifulSoup` and `urllib` \n",
    "\n",
    "   * `urllib` is a standard Python library that functions for requesting data across the web, handling cookies, and even changing metadata such as headers and your user agent. Python documentation for the library : https://docs.python.org/3/library/urllib.html\n",
    " \n",
    " \n",
    "   * `BeautifulSoup` helps format and organize the messy web by fixing bad HTML and presenting us with easily traversable Python objects representing XML structures. Documentation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example with `urllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example with `urllib` and `BeautifulSoup` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Reliably and Handling Exceptions\n",
    "\n",
    "### HTTP and URL Errors\n",
    "   * If the page is not found, an HTTP error will be returned. This HTTP error may be **\"404 Page Not Found\"** or **\"500 Internal Server Error\"**. In thoses cases, the urlopen will throw the generic exception HTTPError. You can handle this exception in the code\n",
    "\n",
    "\n",
    "   * If the server is not found at all (server is down, or the URL is mistyped), urlopen will throw an URLError, which indicates that no server could be reached at all, and, because the remote server is responsible for returning HTTP status codes, an HTTPError cannot be thrown, and the more serious URLError must be caught."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It works !!!\n"
     ]
    }
   ],
   "source": [
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "    \n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "    # return NULL, BREAK, do other thnigs rather than plan B\n",
    "    \n",
    "except URLError as e:\n",
    "    print('The server could not be found!')\n",
    "    \n",
    "else:\n",
    "    print('It works !!!')\n",
    "    # program continues. Note: If you return or break in the\n",
    "    # exception catch, you do not need to use the \"else\" statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Errors (None Tag)\n",
    "\n",
    "Even if we get the page with success, there is still the issue of the content on the page not quite being what we expected. Every time we access a `tag` in a BeautifulSoup object, it’s smart to add a check to make sure the tag actually exists. The objective is to avoid calling methods or parameters of a None object.\n",
    "\n",
    "We always check a tag before using it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag was not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\element.py:1361: UserWarning: .nonExistingTag is deprecated, use .find(\"nonExisting\") instead. If you really were looking for a tag called nonExistingTag, use .find(\"nonExistingTag\")\n",
      "  name=tag_name\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    badContent = bs.nonExistingTag.anotherTag\n",
    "\n",
    "except AttributeError as e:\n",
    "    print('Tag was not found')\n",
    "\n",
    "else:\n",
    "    if badContent == None:\n",
    "        print ('Tag was not found')\n",
    "    else:\n",
    "        print(badContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a function getTitle, which returns either the title of the page, or a None object if there was a problem retrieving it. When writing scrapers, it’s important to think about the overall pattern of the code in order to handle exceptions and make it readable at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    \n",
    "    # check possible http error\n",
    "    except HTTPError as e:\n",
    "        print(\"HTTP ERROR !\")\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    # check possible content (missing tag) error\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(\"AttributeError ERROR !\")\n",
    "        print(e)        \n",
    "        return None\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "title = getTitle('http://www.pythonscraping.com/pages/page1.html')\n",
    "print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebScraping",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
